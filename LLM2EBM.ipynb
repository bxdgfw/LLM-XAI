{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base=\"https://api.jarvis73.com/v1\"\n",
    "openai.organization = os.getenv(\"Organization ID\")\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import t2ebm\n",
    "import guidance\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret import show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/spaceship-titanic/train.csv\") # replace with path to dataset on your machine\n",
    "df.head()\n",
    "\n",
    "dataset_description = \"\"\n",
    "y_axis_description = \"\"\n",
    "\n",
    "# transform cabin since 8000 unique values do not fit into the context windows of the LLM we want to use\n",
    "df['Cabin'] = df['Cabin'].map(lambda x: x[:1] + '/' + x[-1] if isinstance(x, str) else x)\n",
    "\n",
    "# pandas to numpy array\n",
    "X_data = df.drop(columns=[\"PassengerId\", \"Transported\", \"Name\"]).values\n",
    "y_data = df[\"Transported\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_description = \"\"\"\\nHi there, here is a general description of the data set on which I trained the model. This description is from kaggle:\n",
    "\n",
    "Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n",
    "\n",
    "The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n",
    "\n",
    "While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n",
    "\n",
    "To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.\n",
    "\n",
    "The task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.\n",
    "\n",
    "Feature Descriptions:\n",
    "\n",
    "PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "Destination - The planet the passenger will be debarking to.\n",
    "Age - The age of the passenger.\n",
    "VIP - Whether the passenger has paid for special VIP service during the voyage.\n",
    "RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "Name - The first and last names of the passenger.\n",
    "\"\"\"\n",
    "\n",
    "y_axis_description = \"The y-axis depicts contributions in log-odds towards the outcome, that is the probability that the passenger was transported to another dimension.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\interpret\\glassbox\\_ebm\\_ebm.py:669: UserWarning: Missing values detected. Our visualizations do not currently display missing values. To retain the glassbox nature of the model you need to either set the missing values to an extreme value like -1000 that will be visible on the graphs, or manually examine the missing value score in ebm.term_scores_[term_index][0]\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78953421506613"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = df.drop(columns=[\"PassengerId\", \"Transported\", \"Name\"]).columns.tolist()\n",
    "\n",
    "ebm = ExplainableBoostingClassifier(interactions=0, \n",
    "                                    feature_names=feature_names)\n",
    "\n",
    "ebm.fit(X_train, y_train)\n",
    "ebm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importances_to_text(ebm):\n",
    "    feature_importances = \"\"\n",
    "    for feature_idx, feature_name in enumerate(ebm.feature_names_in_):   #得到数据的column： term imortance列表\n",
    "        feature_importances += (\n",
    "            f\"{feature_name}: {ebm.term_importances()[feature_idx]:.2f}\\n\"\n",
    "        )\n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Mars', False, 'F/S', ..., 634.0, 3.0, 1057.0],\n",
       "       ['Earth', False, 'G/S', ..., 0.0, 0.0, 1.0],\n",
       "       ['Earth', True, 'G/S', ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       ['Earth', False, 'E/S', ..., 690.0, 260.0, 5.0],\n",
       "       ['Mars', True, 'F/P', ..., 0.0, 0.0, 0.0],\n",
       "       ['Earth', False, 'G/S', ..., 0.0, 1806.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 函数llm2graph：提取ebm中某个feature的graph，然后通过llmchain来让llm根据该graph的信息对话。\n",
    "1. 将ebm中的graph提取出来，转换成能传给LLM的文本格式\n",
    "2. 将该格式加入prompt模板中并生成prompt\n",
    "3. 根据生成的prompt建立LLMChain并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name= \"gpt-3.5-turbo-16k\",temperature=0)\n",
    "llm_gpt4 = ChatOpenAI(model_name= \"gpt-4\",temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2graph(   \n",
    "        llm,\n",
    "        ebm,\n",
    "        feature_idx,\n",
    "        memory,\n",
    "        num_sentences=7,\n",
    "        dataset_description = None,\n",
    "        y_axis_description = None,\n",
    "):\n",
    "    #返回一个chain，通过chain询问llm对序号为feature_idx的graph\n",
    "    prefix = \"\"\"You are an expert statistician and data scientist.\n",
    "You interpret global explanations produced by a generalized additive model (GAM). GAMs produce explanations in the form of graphs that contain the effect of a specific input feature.\\n\n",
    "\"\"\"\n",
    "    if dataset_description is None or dataset_description == '':\n",
    "        prefix +=\"\"\"You will be given graphs from the model, and the user will ask you questions about the graphs.\"\"\"\n",
    "    else:\n",
    "        prefix +=\"\"\"The user will first provide a general description of the dataset. Then you will be given graphs from the model, and the user will ask you questions about the graphs.\\n\"\"\"\n",
    "    \n",
    "    prefix +=\"\"\"\\n\\nAnswer all questions to the best of your ability, combining both the data contained in the graph\"\"\"\n",
    "    \n",
    "    if dataset_description is not None and len(dataset_description) > 0:\n",
    "        prefix +=\"\"\", the data set description you were given, and your knowledge about the real world.\"\"\"\n",
    "    else:\n",
    "        prefix +=\"\"\" and your knowledge about the real world.\"\"\"\n",
    "    \n",
    "    prefix +=\"\"\"Graphs will be presented as a JSON object with keys representing the x-axis and values representing the y-axis. For continuous features, the keys are intervals that represent ranges where the function predicts the same value. For categorical features, each key represents a possible value that the feature can take.\"\"\"\n",
    "    \n",
    "    if y_axis_description is not None and len(y_axis_description) > 0:\n",
    "        prefix +=y_axis_description\n",
    "    prefix += \"\"\"\\n\\nThe user will provide graphs in the following format:\n",
    "        - The name of the feature depicted in the graph\n",
    "        - The type of the feature (continuous, categorical, or boolean)\n",
    "        - Mean values\n",
    "        - Lower bounds of confidence interval\n",
    "        - Upper bounds of confidence interval\n",
    "    \"\"\"\n",
    "    if dataset_description is not None and len(dataset_description) > 0:\n",
    "        prefix += dataset_description + \"\"\"\\nThe description of dataset ends.\\n\"\"\"\n",
    "    graph = t2ebm.graphs.extract_graph(ebm, feature_idx)\n",
    "    graph = t2ebm.graphs.graph_to_text(graph)\n",
    "    graph = graph.replace(\"{\", \"(\").replace(\"}\", \")\")\n",
    "\n",
    "    suffix=\"\"\"\\nBegin!\n",
    "Human: Consider the following graph from the model.\\n\"\"\"\n",
    "    suffix+=graph\n",
    "    suffix+=\"\"\"\\nAI: I have obtained the information of the graph. You can ask me questions next, and I will answer based on the information,my knowledge about the real world, and maybe the data description.\n",
    "{history}\n",
    "Human: {query}\n",
    "AI:\"\"\"\n",
    "    template=prefix+suffix\n",
    "    prompt = PromptTemplate(input_variables=[\"history\",\"query\"], template=template)\n",
    "    \n",
    "    chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    "    )      \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: HomePlanet  \n",
      "1: CryoSleep  \n",
      "2: Cabin  \n",
      "3: Destination  \n",
      "4: Age  \n",
      "5: VIP  \n",
      "6: RoomService  \n",
      "7: FoodCourt  \n",
      "8: ShoppingMall  \n",
      "9: Spa  \n",
      "10: VRDeck  \n"
     ]
    }
   ],
   "source": [
    "#得到ebm的feature列表以及序号\n",
    "for feature_idx, feature_name in enumerate(ebm.feature_names_in_):  \n",
    "    print(f\"{feature_idx}: {feature_name}  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一个关于0号feature的chain\n",
    "graph = llm2graph(llm,ebm,0,memory=memory,dataset_description=dataset_description,y_axis_description=y_axis_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the graph, we can see that the feature \"HomePlanet\" has three possible values: \"Earth\", \"Europa\", and \"Mars\". The graph shows the effect of each value on the probability of a passenger being transported to an alternate dimension. \\n\\nFrom the means, we can observe that passengers from \"Europa\" have the highest positive effect on the probability, followed by passengers from \"Mars\" with a smaller positive effect. On the other hand, passengers from \"Earth\" have a negative effect on the probability, indicating that they are less likely to be transported to an alternate dimension.\\n\\nThe confidence intervals provide a range of values within which the true effect of each value is likely to fall. For example, the lower bound of the confidence interval for \"Europa\" is 0.786, indicating that the true effect is likely to be at least this large. Similarly, the upper bound of the confidence interval for \"Earth\" is -0.407, indicating that the true effect is likely to be no larger than this value.\\n\\nOverall, the graph suggests that the passenger\\'s home planet is a significant factor in determining whether they were transported to an alternate dimension, with passengers from \"Europa\" being the most likely to be transported.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.run(query=\"Please describe the general pattern of the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The HomePlanet feature has a significant influence on the probability of a passenger being transported to an alternate dimension during the collision with the spacetime anomaly. Passengers from \"Europa\" have the highest positive effect on the probability, indicating that they are more likely to be transported. Passengers from \"Mars\" also have a positive effect, but smaller than those from \"Europa\". On the other hand, passengers from \"Earth\" have a negative effect on the probability, suggesting that they are less likely to be transported. \\n\\nThese findings suggest that the planet of permanent residence plays a role in determining whether a passenger is affected by the anomaly. Passengers from \"Europa\" may have certain characteristics or factors that make them more susceptible to being transported, while passengers from \"Earth\" may have factors that make them less susceptible. The influence of the HomePlanet feature should be taken into account when predicting whether a passenger was transported to an alternate dimension.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.run(query=\"please provide a brief, at most 7 sentence summary of the influence of the feature on the outcome.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the graph and the information provided, changing a passenger\\'s HomePlanet from \"Mars\" to \"Earth\" would result in a decrease in the probability of that passenger being transported to an alternate dimension. Passengers from \"Mars\" have a positive effect on the probability, while passengers from \"Earth\" have a negative effect. Therefore, changing the HomePlanet from \"Mars\" to \"Earth\" would likely decrease the probability of being transported to an alternate dimension. However, the exact magnitude of this change cannot be determined without additional information or the specific model coefficients.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.run(query=\"What will happen if I change one's 'HomePlanet' from 'Mars' to 'earth' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 函数llm2ebm：对于整个ebm通过LLM进行对话\n",
    "1. 通过函数llm2graph_desc获得每个feature的graph description\n",
    "2. 将数据集描述，所有的feature importances，各个feature的graph description加入prompt中提供给llm\n",
    "3. 根据prompt生成LLMChain，进行对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#记忆模块\n",
    "memory_ebm = ConversationBufferMemory(memory_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回某个feature的graph description\n",
    "def llm2graph_desc(   \n",
    "        llm,\n",
    "        ebm,\n",
    "        feature_idx,\n",
    "        dataset_description = None,\n",
    "        y_axis_description = None,\n",
    "        query = \"Now please provide a brief, at most 7 sentence summary of the influence of the feature on the outcome.\"\n",
    "):\n",
    "    prefix = \"\"\"You are an expert statistician and data scientist.\n",
    "You interpret global explanations produced by a generalized additive model (GAM). GAMs produce explanations in the form of graphs that contain the effect of a specific input feature.\\n\n",
    "\"\"\"\n",
    "    if dataset_description is None or dataset_description == '':\n",
    "        prefix +=\"\"\"You will be given graphs from the model, and the user will ask you questions about the graphs.\"\"\"\n",
    "    else:\n",
    "        prefix +=\"\"\"The user will first provide a general description of the dataset. Then you will be given graphs from the model, and the user will ask you questions about the graphs.\\n\"\"\"\n",
    "    \n",
    "    prefix +=\"\"\"\\n\\nAnswer all questions to the best of your ability, combining both the data contained in the graph\"\"\"\n",
    "    \n",
    "    if dataset_description is not None and len(dataset_description) > 0:\n",
    "        prefix +=\"\"\", the data set description you were given, and your knowledge about the real world.\"\"\"\n",
    "    else:\n",
    "        prefix +=\"\"\" and your knowledge about the real world.\"\"\"\n",
    "    \n",
    "    prefix +=\"\"\"Graphs will be presented as a JSON object with keys representing the x-axis and values representing the y-axis. For continuous features, the keys are intervals that represent ranges where the function predicts the same value. For categorical features, each key represents a possible value that the feature can take.\"\"\"\n",
    "    \n",
    "    if y_axis_description is not None and len(y_axis_description) > 0:\n",
    "        prefix +=y_axis_description\n",
    "    prefix += \"\"\"\\n\\nThe user will provide graphs in the following format:\n",
    "        - The name of the feature depicted in the graph\n",
    "        - The type of the feature (continuous, categorical, or boolean)\n",
    "        - Mean values\n",
    "        - Lower bounds of confidence interval\n",
    "        - Upper bounds of confidence interval\n",
    "    \"\"\"\n",
    "    if dataset_description is not None and len(dataset_description) > 0:\n",
    "        prefix += dataset_description + \"\"\"\\nThe description of dataset ends.\\n\"\"\"\n",
    "    graph = t2ebm.graphs.extract_graph(ebm, feature_idx)\n",
    "    graph = t2ebm.graphs.graph_to_text(graph)\n",
    "    graph = graph.replace(\"{\", \"(\").replace(\"}\", \")\")\n",
    "\n",
    "    suffix=\"\"\"\\nBegin!\n",
    "Human: Consider the following graph from the model.\\n\"\"\"\n",
    "    suffix+=graph\n",
    "    suffix+=\"\"\"\\nAI: I have obtained the information of the graph. You can ask me questions next, and I will answer based on the information,my knowledge about the real world, and maybe the data description.\n",
    "Human: {query}\n",
    "AI:\"\"\"\n",
    "    template=prefix+suffix\n",
    "    prompt = PromptTemplate(input_variables=[\"query\"], template=template)\n",
    "    \n",
    "    chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    )\n",
    "    graph_description = chain.run(query=query)\n",
    "    return graph_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HomePlanet',\n",
       " 'CryoSleep',\n",
       " 'Cabin',\n",
       " 'Destination',\n",
       " 'Age',\n",
       " 'VIP',\n",
       " 'RoomService',\n",
       " 'FoodCourt',\n",
       " 'ShoppingMall',\n",
       " 'Spa',\n",
       " 'VRDeck']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebm.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回和ebm对话的LLMchain\n",
    "def llm2ebm(   \n",
    "        llm,\n",
    "        ebm,\n",
    "        memory,\n",
    "        dataset_description = None,\n",
    "        y_axis_description = None,\n",
    "):\n",
    "    feature_importances = feature_importances_to_text(ebm) \n",
    "    graphs = []\n",
    "    graph_descriptions = []\n",
    "    for feature_index in range(len(ebm.feature_names_in_)):       #获取ebm中的所有graph\n",
    "        graphs.append(t2ebm.graphs.extract_graph(ebm, feature_index))\n",
    "    graphs = [t2ebm.graphs.graph_to_text(graph) for graph in graphs]\n",
    "    graph_descriptions = [llm2graph_desc(llm,ebm,idx,dataset_description=dataset_description,y_axis_description=y_axis_description) for idx in range(len(ebm.feature_names_in_)) ]\n",
    "    graph_descriptions = \"\\n\\n\".join(\n",
    "        [\n",
    "            ebm.feature_names_in_[idx] + \": \" + graph_description\n",
    "            for idx, graph_description in enumerate(graph_descriptions)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    prefix = \"\"\"You are an expert statistician and data scientist.\n",
    "            \n",
    "    Your task is to provide an overall summary of a Generalized Additive Model (GAM) and answer the human's questions about it. The model consists of different graphs that contain the effect of a specific input feature.\n",
    "    \n",
    "    You will be given:\n",
    "        - The global feature importances of the different features in the model.\n",
    "        - Summaries of the graphs for the different features in the model. There is exactly one graph for each feature in the model.\n",
    "    \"\"\"\n",
    "    if dataset_description is None or dataset_description == '':\n",
    "        prefix += \"\\n\\nThese inputs will be given to you by the user.\"\n",
    "    else:\n",
    "        prefix += \"\\n\\nThe user will first provide a general description of what the dataset is about. Then you will be given the feature importance scores and the summaries of the individual features.\"\n",
    "    \n",
    "    suffix = \"\"\n",
    "    \n",
    "    if dataset_description is not None and len(dataset_description) > 0:\n",
    "        suffix += \"Human: Here is the general description of the data set\\n\" + dataset_description\n",
    "        suffix += \"\\nAI: Thanks for this general description of the data set. Now please provide the global feature importance.\\n\"\n",
    "    \n",
    "    suffix += \"Human: Here are the global feature importaces.\\n\\n\" + feature_importances + \"\\nAI: Thanks. Now please provide the descriptions of the different graphs.\"\n",
    "    suffix += \"Human: Here are the descriptions of the different graphs.\\n\\n\" + graph_descriptions\n",
    "    suffix+=\"\"\"\\nAI: Thanks. You can ask me questions next.\n",
    "    {history}\n",
    "    Human: {query}\n",
    "    AI:\"\"\"\n",
    "    template=prefix+suffix\n",
    "    prompt = PromptTemplate(input_variables=[\"history\",\"query\"], template=template)\n",
    "    \n",
    "    chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    "    )      \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: The graph of feature RoomService was simplified by 1.1%.\n",
      "INFO: The graph of feature FoodCourt was simplified by 0.4%.\n",
      "INFO: The graph of feature ShoppingMall was simplified by 0.4%.\n",
      "INFO: The graph of feature Spa was simplified by 0.6%.\n",
      "INFO: The graph of feature VRDeck was simplified by 0.6%.\n",
      "INFO: The graph of feature RoomService was simplified by 1.1%.\n",
      "INFO: The graph of feature FoodCourt was simplified by 0.4%.\n",
      "INFO: The graph of feature ShoppingMall was simplified by 0.4%.\n",
      "INFO: The graph of feature Spa was simplified by 0.6%.\n",
      "INFO: The graph of feature VRDeck was simplified by 0.6%.\n"
     ]
    }
   ],
   "source": [
    "#生成和ebm对话的LLMchain\n",
    "talkebm = llm2ebm(llm,ebm,memory_ebm,dataset_description=dataset_description,y_axis_description=y_axis_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Generalized Additive Model (GAM) was trained on a dataset from the Spaceship Titanic, which collided with a spacetime anomaly, transporting almost half of its passengers to an alternate dimension. The global feature importances indicate that the most influential features are Spa, VRDeck, CryoSleep, and RoomService. The graphs for each feature provide insights into their effects on the outcome. Passengers from the planet Europa have the highest probability of being transported, while those from Earth have the lowest. Choosing cryosleep increases the likelihood of being transported. Passengers staying in cabins labeled \"C/S\" and \"C/P\" are more likely to be transported, while those in \"G/P\" and \"A/P\" are less likely. The destination also plays a role, with passengers destined for \"55 Cancri e\" having a higher probability. Age, VIP status, and spending at amenities like the FoodCourt and ShoppingMall also influence the outcome. Overall, the GAM model provides valuable insights into the factors that determine whether a passenger was transported to an alternate dimension during the collision.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talkebm.run(\"Now give a summary about the model at most 10 sentences. Be sure to keep the most important information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, the probability that this person is sent to the alternate dimension can be estimated using the trained GAM model. However, without the specific coefficients and model parameters, it is not possible to provide an accurate probability estimate. To obtain a precise probability, the model would need to be applied to the given feature values using the appropriate calculations.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talkebm.run(\"If a person's HomePlanet is Earth, CryoSleep is False, Cabin is F/4/S, Destination is TRAPPIST-1e, Age is 19.0, VIP is False, RoomService is 0.0, FoodCourt is 9.0, ShoppingMall is 0.0, Spa is 2823.0, VRDeck is 0.0, what is the probability that it is sent to the alternate dimension?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, based on the provided data description, feature importances, and descriptions of each graph, we can make a prediction about the probability of a person being sent to the alternate dimension. \\n\\nThe most influential features in the model are Spa, VRDeck, CryoSleep, and RoomService. However, we also need information about the specific coefficients and model parameters to make an accurate prediction. Without these details, it is not possible to provide a precise probability estimate. \\n\\nTo obtain a reliable prediction, the trained GAM model would need to be applied to the given feature values using the appropriate calculations.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talkebm.run(\"Can you predict based on the provided data description, feature importances and description of each graph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, the probability of this person being transported to the alternate dimension cannot be accurately determined without the specific coefficients and model parameters. To obtain a precise probability estimate, the trained GAM model would need to be applied to the given feature values using the appropriate calculations.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talkebm.run(\"You don't need to make a very precise prediction, you just need to get a probability based on the information I provided. Please tell me the probability of this person being transported to the alternate dimension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the descriptions of the RoomService and ShoppingMall graphs, increasing the value of RoomService from 0 to 50 and ShoppingMall from 0 to 20 would likely have an impact on the probability of being sent to the alternate dimension. \\n\\nFor RoomService, as the amount billed increases, the probability of being transported to an alternate dimension also increases up to a certain point. Therefore, increasing the value of RoomService from 0 to 50 may increase the probability of being sent to the alternate dimension.\\n\\nFor ShoppingMall, the graph suggests that higher spending at the ShoppingMall is associated with a higher probability of being transported to an alternate dimension. Therefore, increasing the value of ShoppingMall from 0 to 20 may also increase the probability of being sent to the alternate dimension.\\n\\nHowever, without the specific coefficients and model parameters, it is not possible to provide an accurate estimate of the impact on the probability. To obtain a precise estimate, the trained GAM model would need to be applied to the given feature values using the appropriate calculations.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talkebm.run(\"If a person's RoomService changes from 0 to 50, and ShoppingMall changes from 0 to 20, what impact will it have on the probability of being sent to the alternate dimension?\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---如果一个人HomePlanet 是Earth，CryoSleep 是False，Cabin是F/4/S ，Destination 是 TRAPPIST-1e，Age 是19.0，VIP是False ，RoomService是0.0, FoodCourt是9.0, ShoppingMall是0.0, Spa是2823.0, VRDeck是0.0，请问它被传送到alternate dimension的概率有多大？\n",
    "\n",
    "---'根据提供的信息，可以使用经过训练的 GAM 模型来估计此人被发送到替代维度的概率。 然而，如果没有特定的系数和模型参数，就不可能提供准确的概率估计。 为了获得精确的概率，需要使用适当的计算将该模型应用于给定的特征值。\n",
    "\n",
    "--\r\n",
    "你能根据提供的data description，feature importances和各个graph的descriptio预测n来\n",
    "测\n",
    "---是的，根据提供的数据描述、特征重要性以及每个图的描述，我们可以预测一个人被发送到另一个维度的概率。 \\n\\n模型中最有影响力的功能是 Spa、VRDeck、CryoSleep 和 RoomService。 然而，我们还需要有关特定系数和模型参数的信息才能做出准确的预测。 如果没有这些详细信息，就不可能提供精确的概率估计。 \\n\\n为了获得可靠的预测，需要使用适当的计算将经过训练的 GAM 模型应用于给定征值。。\n",
    "--吗\n",
    "-你不需要做出非常精准的预测，你只需要根据我提供的这些信息来得到一个概率。请告诉我这个人被传送到alternate dimension的概n的\n",
    "---‘根据提供的信息，如果没有具体的系数和模型参数，就无法准确确定此人被传送到异次元的概率。 为了获得精确的概率估计，需要使用适当的计算将经过训练的 GAM 模型应用于给特征值。率。---\r\n",
    "\r\n",
    "如果将一个人的roomservice从0变成50，shoppingMall从0变成20，将会对他被传送到alternate dimensio什么\n",
    "的影n----根据 RoomService 和 ShoppingMall 图表的描述，将 RoomService 的值从 0 增加到 50，将 ShoppingMall 的值从 0 增加到 20，可能会对发送到替代维度的概率产生影响。 \n",
    "对于 RoomService 来说，随着计费金额的增加，被传送到异次元的概率也会增加到一定程度。 因此，将 RoomService 的值从 0 增加到 50 可能会增加被发送到备用维度的概率。\n",
    "对于 ShoppingMall，该图表明，在 ShoppingMall 的支出越高，被发送到备用维度的概率越高 方面。 因此，将 ShoppingMall 的值从 0 增加到 20 也可能会增加被发送到替代维度的概率。\n",
    "但是，在没有具体系数和模型参数的情况下，无法估计对 概率。 为了获得精确的估计，需要使用适当的计算将经过训练的 GAM 模型应用于给定的特征值。增加被值。什么样的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: The graph of feature RoomService was simplified by 1.1%.\n",
      "INFO: The graph of feature FoodCourt was simplified by 0.4%.\n",
      "INFO: The graph of feature ShoppingMall was simplified by 0.4%.\n",
      "INFO: The graph of feature Spa was simplified by 0.6%.\n",
      "INFO: The graph of feature VRDeck was simplified by 0.6%.\n",
      "INFO: The graph of feature RoomService was simplified by 1.1%.\n",
      "INFO: The graph of feature FoodCourt was simplified by 0.4%.\n",
      "INFO: The graph of feature ShoppingMall was simplified by 0.4%.\n",
      "INFO: The graph of feature Spa was simplified by 0.6%.\n",
      "INFO: The graph of feature VRDeck was simplified by 0.6%.\n"
     ]
    }
   ],
   "source": [
    "#使用gpt4\n",
    "talkebm = llm2ebm(llm_gpt4,ebm,memory_ebm,dataset_description=dataset_description,y_axis_description=y_axis_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the descriptions of the graphs for each feature, we can make a qualitative prediction. \\n\\nThe person is from Earth, which has a negative contribution to the log-odds of being transported to an alternate dimension. They are not in CryoSleep, which also contributes negatively. The cabin type is not specified in the graph descriptions, so its effect is unclear. The destination, TRAPPIST-1e, contributes negatively. Being 19 years old seems to have a negative contribution as well, as the age graph shows a decrease in log-odds for this age group. The person is not a VIP, which has a near-zero contribution. RoomService is 0, which is in the range that contributes positively. FoodCourt is 9, which is not clearly specified in the graph description, but lower values generally contribute negatively. ShoppingMall is 0, which contributes negatively. Spa is 2823, which is likely to contribute negatively as higher values of Spa usage are associated with a lower likelihood of being transported. VRDeck is 0, which is not clearly specified in the graph description, but higher values generally contribute negatively.\\n\\nGiven these factors, it seems that the person is less likely to be transported to an alternate dimension. However, without the specific coefficients and model parameters, it is not possible to provide an accurate probability estimate. To obtain a precise probability, the trained GAM model would need to be applied to the given feature values using the appropriate calculations.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talkebm.run(\"If a person's HomePlanet is Earth, CryoSleep is False, Cabin is F/4/S, Destination is TRAPPIST-1e, Age is 19.0, VIP is False, RoomService is 0.0, FoodCourt is 9.0, ShoppingMall is 0.0, Spa is 2823.0, VRDeck is 0.0, what is the probability that it is sent to the alternate dimension?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the descriptions of the graphs for each feature, we can make a qualitative prediction. \\n\\nThe person's HomePlanet changing from Earth to Mars would likely increase the probability of being transported to an alternate dimension. This is because the graph for HomePlanet shows that passengers from Mars have a slight positive contribution to the log-odds of being transported, while passengers from Earth have a negative contribution.\\n\\nThe person's Spa usage changing from 2823 to 20 would also likely increase the probability of being transported to an alternate dimension. This is because the graph for Spa shows that lower values of Spa usage contribute positively to the log-odds of being transported, while higher values contribute negatively.\\n\\nHowever, without the specific coefficients and model parameters, it is not possible to provide an accurate estimate of the impact on the probability. To obtain a precise estimate, the trained GAM model would need to be applied to the given feature values using the appropriate calculations.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talkebm.run(\"If his Spa changes from 2823 to 20 and HomePlanet changes from Earth to Mars, what impact will it have on his probability of being teleported to the alternate dimension?\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---如果一个人HomePlanet 是Earth，CryoSleep 是False，Cabin是F/4/S ，Destination 是 TRAPPIST-1e，Age 是19.0，VIP是False ，RoomService是0.0, FoodCourt是9.0, ShoppingMall是0.0, Spa是2823.0, VRDeck是0.0，请问它被传送到alternate dimension的概率有多大\n",
    "\n",
    "---“根据每个特征的图表描述，我们可以做出定性预测。 \\n\\n这个人来自地球，这对传送到另一个维度的对数几率有负面贡献。 它们不在 CryoSleep 中，这也会产生负面影响。 图中没有具体说明舱型，因此其效果尚不清楚。 目的地 TRAPPIST-1e 产生负面影响。 19 岁似乎也有负贡献，因为年龄图显示该年龄组的对数赔率下降。 此人不是 VIP，其贡献几乎为零。 RoomService 为 0，处于积极贡献的范围内。 FoodCourt 为 9，在图表描述中没有明确指定，但较低的值通常会产生负面影响。 ShoppingMall 为 0，贡献为负。 Spa 为 2823，这可能会产生负面影响，因为较高的 Spa 使用值与较低的运输可能性相关。 VRDeck 为 0，在图形描述中没有明确指定，但较高的值通常会产生负面影响。\\n\\n考虑到这些因素，似乎该人不太可能被传送到另一个维度。 然而，如果没有特定的系数和模型参数，就不可能提供准确的概率估计。 为了获得精确的概率，需要使用适当的计算将经过训练的 GAM 模型应用于给定的特征值。\n",
    "\n",
    "---如果他的Spa从2823改变成20，HomePlanet 从Earth改变成Mars，将会对他被传送到alternate dimension的概率产生什么样的影响?\n",
    "\n",
    "---“根据每个特征的图表描述，我们可以做出定性预测。\n",
    "人的家乡星球从地球变为火星，很可能会增加被传送到另一个维度的概率。这是因为家乡星球的图表 显示来自火星的乘客对被运送的对数几率有轻微的正贡献，而来自地球的乘客则有负贡献。\n",
    "该人的 Spa 使用量从 2823 更改为 20 也可能会增加被运送到的概率 另一个维度。这是因为 Spa 的图表显示，Spa 使用率的较低值对运输的对数几率有正向贡献，而较高的值则有负向贡献。\n",
    "但是，如果没有特定系数和模型参数，则 不可能提供对概率的影响的准确估计。为了获得精确的估计，需要使用适当的计算将经过训练的 GAM 模型应用于给定的特征值。”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import ZeroShotAgent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name= \"gpt-3.5-turbo-16k\",temperature=0)\n",
    "llm_gpt4 = ChatOpenAI(model_name= \"gpt-4\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part of tool description\n",
    "\"\"\"\n",
    "\"For example, the question is if a persion's Age is 27,WorkClass is Private,fnlwgt is 160178,Education \"\n",
    "    \"is Some-college, EducationNum is 10, MaritalStatus is Divorced ,Occupation is Adm-clerical,what is the prediction.\"\n",
    "    \"you need to provide this tool with [27,' Private', 160178,' Some-college', 10, ' Divorced', ' Adm-clerical']\"\n",
    "\n",
    "\n",
    "    \"For example, the question is if a person's HomePlanet is Earth, CryoSleep is False, Cabin is F/4/S,\"\n",
    "    \" Destination is TRAPPIST-1e, Age is 19.0, VIP is False, RoomService is 0.0, FoodCourt\"\n",
    "    \" is 9.0, ShoppingMall is 0.0, Spa is 2823.0, VRDeck is 0.0, what is the probability\"\n",
    "    \" that it is sent to the alternate dimension? you need to provide this tool with \"\n",
    "    \"['Earth','False','F/4/S','TRAPPIST-1e',19.0,False,0.0,9.0,0.0,2823.0,0.0].\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = (\n",
    "    \"use this tool when you need to predict the probabilities given a series of feature values\"\n",
    "    \"in a Generalized Additive Model. It will return the final probability.To use the tool\"\n",
    "    \" you must provide a list consisting of the value of each feature.The value must be provided in question.\"\n",
    "    \"For example, the question is if a persion's Age is 27,WorkClass is Private,fnlwgt is 160178,Education \"\n",
    "    \"is Some-college, EducationNum is 10, MaritalStatus is Divorced ,Occupation is Adm-clerical,what is the prediction.\"\n",
    "    \"you need to provide this tool with [27,' Private', 160178,' Some-college', 10, ' Divorced', ' Adm-clerical']\"\n",
    "    #\"['adjacent_side', 'opposite_side', 'angle'].\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(input: np.ndarray):\n",
    "    sample = eval(input)\n",
    "    return ebm.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_tool = Tool(\n",
    "    name='Forecast',\n",
    "    func=forecast,\n",
    "    description=desc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[]\n",
    "tools.append(forecast_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: The graph of feature RoomService was simplified by 1.1%.\n",
      "INFO: The graph of feature FoodCourt was simplified by 0.4%.\n",
      "INFO: The graph of feature ShoppingMall was simplified by 0.4%.\n",
      "INFO: The graph of feature Spa was simplified by 0.6%.\n",
      "INFO: The graph of feature VRDeck was simplified by 0.6%.\n"
     ]
    }
   ],
   "source": [
    "#和ebm对话的prompt生成\n",
    "feature_importances = feature_importances_to_text(ebm) \n",
    "graphs = []\n",
    "graph_descriptions = []\n",
    "for feature_index in range(len(ebm.feature_names_in_)):       #获取ebm中的所有graph\n",
    "    graphs.append(t2ebm.graphs.extract_graph(ebm, feature_index))\n",
    "graphs = [t2ebm.graphs.graph_to_text(graph) for graph in graphs]\n",
    "'''graph_descriptions = [llm2graph_desc(llm,ebm,idx,dataset_description=dataset_description,y_axis_description=y_axis_description) for idx in range(len(ebm.feature_names_in_)) ]\n",
    "graph_descriptions = \"\\n\\n\".join(\n",
    "    [\n",
    "        ebm.feature_names_in_[idx] + \": \" + graph_description\n",
    "        for idx, graph_description in enumerate(graph_descriptions)\n",
    "    ]\n",
    ")'''\n",
    "\n",
    "prefix = \"\"\"You are an expert statistician and data scientist.\n",
    "        \n",
    "Your task is complete some tasks about a Generalized Additive Model (GAM). The model consists of different graphs that contain the effect of a specific input feature.\n",
    "You are bad at math such as predict the probability. So you must directly use the tool when asked about math problem. \n",
    "\n",
    "You will be given:\n",
    "    - The global feature importances of the different features in the model.\n",
    "    - Summaries of the graphs for the different features in the model. There is exactly one graph for each feature in the model.\n",
    "\"\"\"\n",
    "if dataset_description is None or dataset_description == '':\n",
    "    prefix += \"\\n\\nThese inputs will be given to you by the user.\"\n",
    "else:\n",
    "    prefix += \"\\n\\nThe user will first provide a general description of what the dataset is about. Then you will be given the feature importance scores and the summaries of the individual features.\"\n",
    "\n",
    "suffix = \"\"\n",
    "\n",
    "if dataset_description is not None and len(dataset_description) > 0:\n",
    "    suffix += \"Here is the general description of the data set\\n\" + dataset_description\n",
    "    \n",
    "suffix += \"\\nHere are the global feature importances.Be sure not to provide these importances for the tool directly for prediction.\\n\\n\" + feature_importances \n",
    "#suffix += \"\\nHere are the descriptions of the different graphs.\\n\\n\"\n",
    "#suffix += graph_descriptions\n",
    "suffix+=\"\"\"\n",
    "Complete the objective as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of {tool_names}\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "template = prefix + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template 按照定义的 template，将需要的值都填写进去\n",
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        return [HumanMessage(content=formatted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\",\"history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ag = ConversationBufferMemory(memory_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#解析 llm 的输出，根据输出文本找到需要执行的决策。\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            new_out = llm_output.split(\"Action:\")[0]\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": new_out.split(\":\")[1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "            #raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory_ag ,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To summarize the model, I need to highlight the important information about the Generalized Additive Model (GAM) based on the given feature importances and summaries of the individual features.\n",
      "\n",
      "Action: I will start by mentioning that the GAM consists of different graphs that represent the effect of each input feature on the prediction. The feature importances indicate the relative importance of each feature in the model, with Spa having the highest importance (0.75) followed by VRDeck (0.66). CryoSleep (0.52) and RoomService (0.52) also have relatively high importances.\n",
      "\n",
      "Action Input: None\n",
      "\u001b[0m\n",
      "\n",
      "Observation:I will start by mentioning that the GAM consists of different graphs that represent the effect of each input feature on the prediction. The feature importances indicate the relative importance of each feature in the model, with Spa having the highest importance (0.75) followed by VRDeck (0.66). CryoSleep (0.52) and RoomService (0.52) also have relatively high importances. is not a valid tool, try one of [Calculator, Forecast].\n",
      "\u001b[32;1m\u001b[1;3mI need to provide a summary of the individual feature summaries as well. I will mention that the HomePlanet feature has an importance of 0.47 and its graph shows a positive relationship with the prediction. The CryoSleep feature has an importance of 0.52 and its graph shows a negative relationship with the prediction. The Cabin feature has an importance of 0.39 and its graph shows a non-linear relationship with the prediction. The Destination feature has an importance of 0.15 and its graph shows a weak positive relationship with the prediction. The Age feature has an importance of 0.11 and its graph shows a non-linear relationship with the prediction. The VIP feature has a low importance of 0.01 and its graph shows a weak positive relationship with the prediction. The RoomService feature has an importance of 0.52 and its graph shows a positive relationship with the prediction. The FoodCourt feature has an importance of 0.31 and its graph shows a positive relationship with the prediction. The ShoppingMall feature has an importance of 0.13 and its graph shows a weak positive relationship with the prediction. The Spa feature has the highest importance of 0.75 and its graph shows a strong positive relationship with the prediction. The VRDeck feature has an importance of 0.66 and its graph shows a strong positive relationship with the prediction.\n",
      "\n",
      "Final Answer: The Generalized Additive Model (GAM) consists of different graphs representing the effect of each input feature on the prediction. The feature importances indicate the relative importance of each feature, with Spa and VRDeck having the highest importances. The individual feature summaries show the relationships between each feature and the prediction, with some features having non-linear relationships.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Generalized Additive Model (GAM) consists of different graphs representing the effect of each input feature on the prediction. The feature importances indicate the relative importance of each feature, with Spa and VRDeck having the highest importances. The individual feature summaries show the relationships between each feature and the prediction, with some features having non-linear relationships.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"Now give a summary about the model at most 10 sentences. Be sure to keep the most important information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: We need to use the Forecast tool to predict the probability.\n",
      "\n",
      "Action: Forecast\n",
      "Action Input: ['Earth', False, 'F/4/S', 'TRAPPIST-1e', 19.0, False, 0.0, 9.0, 0.0, 2823.0, 0.0]\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3m[False]\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe probability that the person is sent to the alternate dimension is False.\n",
      "\n",
      "Final Answer: False\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"If a person's HomePlanet is Earth, CryoSleep is False, Cabin is F/4/S, Destination is TRAPPIST-1e, Age is 19.0, VIP is False, RoomService is 0.0, FoodCourt is 9.0, ShoppingMall is 0.0, Spa is 2823.0, VRDeck is 0.0, what is the probability that it is sent to the alternate dimension?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The answer to the last question is False.\n",
      "\n",
      "Final Answer: False\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"what is the answer of the last question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert statistician and data scientist.\n",
      "        \n",
      "Your task is complete some tasks about a Generalized Additive Model (GAM). The model consists of different graphs that contain the effect of a specific input feature.\n",
      "You are bad at math such as predict the probability. So you must directly use the tool when asked about math problem. \n",
      "\n",
      "You will be given:\n",
      "    - The global feature importances of the different features in the model.\n",
      "    - Summaries of the graphs for the different features in the model. There is exactly one graph for each feature in the model.\n",
      "\n",
      "\n",
      "The user will first provide a general description of what the dataset is about. Then you will be given the feature importance scores and the summaries of the individual features.Here is the general description of the data set\n",
      "\n",
      "Hi there, here is a general description of the data set on which I trained the model. This description is from kaggle:\n",
      "\n",
      "Welcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n",
      "\n",
      "The Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n",
      "\n",
      "While rounding Alpha Centauri en route to its first destination—the torrid 55 Cancri E—the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n",
      "\n",
      "To help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship’s damaged computer system.\n",
      "\n",
      "The task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.\n",
      "\n",
      "Feature Descriptions:\n",
      "\n",
      "PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
      "HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n",
      "CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
      "Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
      "Destination - The planet the passenger will be debarking to.\n",
      "Age - The age of the passenger.\n",
      "VIP - Whether the passenger has paid for special VIP service during the voyage.\n",
      "RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
      "Name - The first and last names of the passenger.\n",
      "\n",
      "Here are the global feature importances.Be sure not to provide these importances for the tool directly for prediction.\n",
      "\n",
      "HomePlanet: 0.47\n",
      "CryoSleep: 0.52\n",
      "Cabin: 0.39\n",
      "Destination: 0.15\n",
      "Age: 0.11\n",
      "VIP: 0.01\n",
      "RoomService: 0.52\n",
      "FoodCourt: 0.31\n",
      "ShoppingMall: 0.13\n",
      "Spa: 0.75\n",
      "VRDeck: 0.66\n",
      "\n",
      "Complete the objective as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of {tool_names}\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "{history}\n",
      "\n",
      "Question: {input}\n",
      "{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "#显示agent的prompt template\n",
    "print(agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: The graph of feature RoomService was simplified by 1.1%.\n",
      "INFO: The graph of feature FoodCourt was simplified by 0.4%.\n",
      "INFO: The graph of feature ShoppingMall was simplified by 0.4%.\n",
      "INFO: The graph of feature Spa was simplified by 0.6%.\n",
      "INFO: The graph of feature VRDeck was simplified by 0.6%.\n"
     ]
    }
   ],
   "source": [
    "#和ebm对话的prompt生成\n",
    "feature_importances = feature_importances_to_text(ebm) \n",
    "graphs = []\n",
    "graph_descriptions = []\n",
    "for feature_index in range(len(ebm.feature_names_in_)):       #获取ebm中的所有graph\n",
    "    graphs.append(t2ebm.graphs.extract_graph(ebm, feature_index))\n",
    "graphs = [t2ebm.graphs.graph_to_text(graph) for graph in graphs]\n",
    "\n",
    "prefix = \"\"\"You are an expert statistician and data scientist.\n",
    "        \n",
    "Your task is to provide an overall summary of a Generalized Additive Model (GAM). The model consists of different graphs that contain the effect of a specific input feature.\n",
    "\n",
    "You will be given:\n",
    "    - The global feature importances of the different features in the model.\n",
    "    - Summaries of the graphs for the different features in the model. There is exactly one graph for each feature in the model.\n",
    "\"\"\"\n",
    "if dataset_description is None or dataset_description == '':\n",
    "    prefix += \"\\n\\nThese inputs will be given to you by the user.\"\n",
    "else:\n",
    "    prefix += \"\\n\\nThe user will first provide a general description of what the dataset is about. Then you will be given the feature importance scores and the summaries of the individual features.\"\n",
    "\n",
    "suffix = \"\"\n",
    "\n",
    "if dataset_description is not None and len(dataset_description) > 0:\n",
    "    suffix += \"Here is the general description of the data set\\n\" + dataset_description\n",
    "    \n",
    "suffix += \"Here are the global feature importaces.\\n\\n\" + feature_importances \n",
    "suffix += \"\\nHere are the descriptions of the different graphs.\\n\\n\" + \"You have access to the following tools:\"\n",
    "prefix = prefix + suffix\n",
    "suffix=\"\"\"\n",
    "Begin!\n",
    "{chat_history}\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt2 = ZeroShotAgent.create_prompt(\n",
    "    tools,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=ChatOpenAI(temperature=0), prompt=prompt)\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"If a person's HomePlanet is Earth, CryoSleep is False, Cabin is F/4/S, Destination is TRAPPIST-1e, Age is 19.0, VIP is False, RoomService is 0.0, FoodCourt is 9.0, ShoppingMall is 0.0, Spa is 2823.0, VRDeck is 0.0, what is the probability that it is sent to the alternate dimension?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AgentActionMessageLog' from 'langchain.schema.agent' (D:\\anaconda3\\lib\\site-packages\\langchain\\schema\\agent.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat_scratchpad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_to_openai_functions\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIFunctionsAgentOutputParser\n\u001b[0;32m      5\u001b[0m \u001b[38;5;129m@tool\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_word_length\u001b[39m(word: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\format_scratchpad\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat_scratchpad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_log_to_str\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat_scratchpad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog_to_messages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_log_to_messages\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat_scratchpad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     format_to_openai_functions,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat_scratchpad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_xml\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_xml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_to_openai_functions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_log_to_str\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_log_to_messages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m ]\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\langchain\\agents\\format_scratchpad\\openai_functions.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Sequence, Tuple\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentAction, AgentActionMessageLog\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIMessage, BaseMessage, FunctionMessage\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_agent_action_to_messages\u001b[39m(\n\u001b[0;32m      9\u001b[0m     agent_action: AgentAction, observation: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m     10\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseMessage]:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AgentActionMessageLog' from 'langchain.schema.agent' (D:\\anaconda3\\lib\\site-packages\\langchain\\schema\\agent.py)"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [get_word_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are very powerful assistant, but bad at calculating lengths of words.\"),\n",
    "    MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "llm_with_tools = llm.bind(\n",
    "    functions=[format_tool_to_openai_function(t) for t in tools]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAIFunctionsAgentOutputParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: format_to_openai_functions(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintermediate_steps\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m } \u001b[38;5;241m|\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm_with_tools \u001b[38;5;241m|\u001b[39m \u001b[43mOpenAIFunctionsAgentOutputParser\u001b[49m()\n\u001b[0;32m      6\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OpenAIFunctionsAgentOutputParser' is not defined"
     ]
    }
   ],
   "source": [
    "agent = {\n",
    "    \"input\": lambda x: x[\"input\"],\n",
    "    \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps']),\n",
    "    \"chat_history\": lambda x: x[\"chat_history\"]\n",
    "} | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True，memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"how many letters in the word educa?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magent_use\u001b[39m(\n\u001b[0;32m      2\u001b[0m     query,\n\u001b[1;32m----> 3\u001b[0m     chat_history\u001b[38;5;241m=\u001b[39m\u001b[43mchat_history\u001b[49m\n\u001b[0;32m      4\u001b[0m ):\n\u001b[0;32m      5\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow many letters in the word educa?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     result \u001b[38;5;241m=\u001b[39m agent_executor\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: input1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: chat_history})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chat_history' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "agent_kwargs = {\n",
    "    \"extra_prompt_messages\": [MessagesPlaceholder(variable_name=\"memory\")],\n",
    "}\n",
    "memory = ConversationBufferMemory(memory_key=\"memory\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    "    agent_kwargs=agent_kwargs,\n",
    "    memory=memory,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input', 'memory'], output_parser=None, partial_variables={}, messages=[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}), MessagesPlaceholder(variable_name='memory'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='{input}', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.agent.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAIFunctionsAgent' object has no attribute 'llm_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mtemplate\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OpenAIFunctionsAgent' object has no attribute 'llm_chain'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: The graph of feature RoomService was simplified by 1.1%.\n",
      "INFO: The graph of feature FoodCourt was simplified by 0.4%.\n",
      "INFO: The graph of feature ShoppingMall was simplified by 0.4%.\n",
      "INFO: The graph of feature Spa was simplified by 0.6%.\n",
      "INFO: The graph of feature VRDeck was simplified by 0.6%.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_importances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     suffix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman: Here is the general description of the data set\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_description\n\u001b[0;32m     28\u001b[0m     suffix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAI: Thanks for this general description of the data set. Now please provide the global feature importance.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 30\u001b[0m suffix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman: Here are the global feature importaces.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mfeature_importances\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAI: Thanks. Now please provide the descriptions of the different graphs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m suffix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman: Here are the descriptions of the different graphs.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m suffix\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAI: Thanks. You can ask me questions next.\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;132;01m{history}\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124mHuman: \u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124mAI:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_importances' is not defined"
     ]
    }
   ],
   "source": [
    "#和ebm对话的prompt生成\n",
    "\n",
    "graphs = []\n",
    "graph_descriptions = []\n",
    "for feature_index in range(len(ebm.feature_names_in_)):       #获取ebm中的所有graph\n",
    "    graphs.append(t2ebm.graphs.extract_graph(ebm, feature_index))\n",
    "graphs = [t2ebm.graphs.graph_to_text(graph) for graph in graphs]\n",
    "#graph_descriptions = [llm2graph_desc(llm,ebm,idx,dataset_description=dataset_description,y_axis_description=y_axis_description) for idx in range(len(ebm.feature_names_in_)) ]\n",
    "\n",
    "\n",
    "prefix = \"\"\"You are an expert statistician and data scientist.\n",
    "        \n",
    "Your task is to provide an overall summary of a Generalized Additive Model (GAM). The model consists of different graphs that contain the effect of a specific input feature.\n",
    "\n",
    "You will be given:\n",
    "    - The global feature importances of the different features in the model.\n",
    "    - Summaries of the graphs for the different features in the model. There is exactly one graph for each feature in the model.\n",
    "\"\"\"\n",
    "if dataset_description is None or dataset_description == '':\n",
    "    prefix += \"\\n\\nThese inputs will be given to you by the user.\"\n",
    "else:\n",
    "    prefix += \"\\n\\nThe user will first provide a general description of what the dataset is about. Then you will be given the feature importance scores and the summaries of the individual features.\"\n",
    "\n",
    "suffix = \"\"\n",
    "\n",
    "if dataset_description is not None and len(dataset_description) > 0:\n",
    "    suffix += \"Human: Here is the general description of the data set\\n\" + dataset_description\n",
    "    suffix += \"\\nAI: Thanks for this general description of the data set. Now please provide the global feature importance.\\n\"\n",
    "\n",
    "suffix += \"Human: Here are the global feature importaces.\\n\\n\" + feature_importances + \"\\nAI: Thanks. Now please provide the descriptions of the different graphs.\"\n",
    "suffix += \"Human: Here are the descriptions of the different graphs.\\n\\n\"\n",
    "suffix+=\"\"\"\\nAI: Thanks. You can ask me questions next.\n",
    "{history}\n",
    "Human: {query}\n",
    "AI:\"\"\"\n",
    "\n",
    "print(prefix+suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2graph(   \n",
    "        llm,\n",
    "        ebm,\n",
    "        feature_idx,\n",
    "        memory,\n",
    "        num_sentences=7,\n",
    "        dataset_description = None,\n",
    "        y_axis_description = None,\n",
    "):\n",
    "    prefix = \"\"\"You are an expert statistician and data scientist.\n",
    "You interpret global explanations produced by a generalized additive model (GAM). GAMs produce explanations in the form of graphs that contain the effect of a specific input feature.\\n\n",
    "\"\"\"\n",
    "    if dataset_description is None or dataset_description == '':\n",
    "        prefix +=\"\"\"You will be given graphs from the model, and the user will ask you questions about the graphs.\"\"\"\n",
    "    else:\n",
    "        prefix +=\"\"\"The user will first provide a general description of the dataset. Then you will be given graphs from the model, and the user will ask you questions about the graphs.\\n\"\"\"\n",
    "    \n",
    "    prefix +=\"\"\"\\n\\nAnswer all questions to the best of your ability, combining both the data contained in the graph\"\"\"\n",
    "    \n",
    "    if dataset_description is not None and len(dataset_description) > 0:\n",
    "        prefix +=\"\"\", the data set description you were given, and your knowledge about the real world.\"\"\"\n",
    "    else:\n",
    "        prefix +=\"\"\" and your knowledge about the real world.\"\"\"\n",
    "    \n",
    "    prefix +=\"\"\"Graphs will be presented as a JSON object with keys representing the x-axis and values representing the y-axis. For continuous features, the keys are intervals that represent ranges where the function predicts the same value. For categorical features, each key represents a possible value that the feature can take.\"\"\"\n",
    "    \n",
    "    if y_axis_description is not None and len(y_axis_description) > 0:\n",
    "        prefix +=y_axis_description\n",
    "    prefix += \"\"\"\\n\\nThe user will provide graphs in the following format:\n",
    "        - The name of the feature depicted in the graph\n",
    "        - The type of the feature (continuous, categorical, or boolean)\n",
    "        - Mean values\n",
    "        - Lower bounds of confidence interval\n",
    "        - Upper bounds of confidence interval\n",
    "    \"\"\"\n",
    "    graph = t2ebm.graphs.extract_graph(ebm, feature_idx)\n",
    "    graph = t2ebm.graphs.graph_to_text(graph)\n",
    "    graph = graph.replace(\"{\", \"(\").replace(\"}\", \")\")\n",
    "\n",
    "    suffix=\"\"\"\\nBegin!\n",
    "Human: Consider the following graph from the model.\\n\"\"\"\n",
    "    suffix+=graph\n",
    "    suffix+=\"\"\"\\nAI: I have obtained the information of the graph. You can ask me questions next, and I will answer based on the information,my knowledge about the real world, and maybe the data description.\n",
    "{history}\n",
    "Human: {query}\n",
    "AI:\"\"\"\n",
    "    template=prefix+suffix\n",
    "    prompt = PromptTemplate(input_variables=[\"history\",\"query\"], template=template)\n",
    "    \n",
    "    chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    )      \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2ebm(   #用来测试，不执行\n",
    "        llm,\n",
    "        ebm,\n",
    "        feature_idx,\n",
    "        memory,\n",
    "        num_sentences=7,\n",
    "        dataset_description = None,\n",
    "        y_axis_description = None,\n",
    "        query = \"\",\n",
    "):\n",
    "    prefix = \"\"\"You are an expert statistician and data scientist.\n",
    "You interpret global explanations produced by a generalized additive model (GAM). GAMs produce explanations in the form of graphs that contain the effect of a specific input feature.\n",
    "\"\"\"\n",
    "if dataset_description is None or dataset_description == '':\n",
    "    prefix += \"\"\"You will be given graphs from the model, and the user will ask you questions about the graphs.\"\"\"\n",
    "else:\n",
    "    prefix +=\"\"\"The user will first provide a general description of the dataset. Then you will be given graphs from the model, and the user will ask you questions about the graphs.\\n\"\"\"\n",
    "\n",
    "prefix += \"\"\"\\nAnswer all questions to the best of your ability, combining both the data contained in the graph\"\"\"\n",
    "\n",
    "if dataset_description is not None and len(dataset_description) > 0:\n",
    "    prefix +=\"\"\", the data set description you were given, and your knowledge about the real world.\"\"\"\n",
    "else:\n",
    "    prefix +=\"\"\" and your knowledge about the real world.\"\"\"\n",
    "\n",
    "prefix +=\"\"\"Graphs will be presented as a JSON object with keys representing the x-axis and values representing the y-axis. For continuous features, the keys are intervals that represent ranges where the function predicts the same value. For categorical features, each key represents a possible value that the feature can take.\"\"\"\n",
    "\n",
    "if y_axis_description is not None and len(y_axis_description) > 0:\n",
    "    prefix +=\"\"\"{y_axis_description}\"\"\"\n",
    "prefix += \"\"\"\\nThe user will provide graphs in the following format:\n",
    "    - The name of the feature depicted in the graph\n",
    "    - The type of the feature (continuous, categorical, or boolean)\n",
    "    - Mean values\n",
    "    - Lower bounds of confidence interval\n",
    "    - Upper bounds of confidence interval\n",
    "\"\"\"\n",
    "\n",
    "    graph = t2ebm.graphs.extract_graph(ebm, feature_idx)\n",
    "    graph = t2ebm.graphs.graph_to_text(graph)\n",
    "    graph = graph.replace(\"{\", \"(\").replace(\"}\", \")\")\n",
    "\n",
    "    suffix=\"\"\"\\nBegin!\n",
    "Human: Consider the following graph from the model.\\n\"\"\"\n",
    "    suffix+=graph\n",
    "    suffix+=\"\"\"\\nAI: I have obtained the information of the graph. You can ask me questions next, and I will answer based on the information,my knowledge about the real world, and maybe the data description.\n",
    "{history}\n",
    "Human: {query}\n",
    "AI:\"\"\"\n",
    "    template=prefix+suffix\n",
    "    prompt = PromptTemplate(input_variables=[\"history\",\"query\"], template=template)\n",
    "    print(template)\n",
    "    chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    )      \n",
    "    print(chain.run(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "\"\"\"You are an expert statistician and data scientist.\n",
    "You interpret global explanations produced by a generalized additive model (GAM). GAMs produce explanations in the form of graphs that contain the effect of a specific input feature.\n",
    "You will be given graphs from the model, and the user will ask you questions about the graphs.\n",
    "Answer all questions to the best of your ability, combining both the data contained in the graph and your knowledge about the real world.Graphs will be presented as a JSON object with keys representing the x-axis and values representing the y-axis. For continuous features, the keys are intervals that represent ranges where the function predicts the same value. For categorical features, each key represents a possible value that the feature can take.\n",
    "The user will provide graphs in the following format:\n",
    "    - The name of the feature depicted in the graph\n",
    "    - The type of the feature (continuous, categorical, or boolean)\n",
    "    - Mean values\n",
    "    - Lower bounds of confidence interval\n",
    "    - Upper bounds of confidence interval\n",
    "    \"\"\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Consider the following graph from the model.\n",
    "{graph}\n",
    "Please describe the general pattern of the graph at most 7 sentence.\n",
    "\"\"\")\n",
    "    ]\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"history\",return_messages=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "result = llm_chain.run(\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSocktastic!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.run(\"colorful socks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "{chat_history}\n",
    "Human:{human_input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"chat_history\",\"human_input\"], template=template)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "\n",
    "chat_llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "[HumanMessage(content='Hi there my friend', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False), HumanMessage(content='Not too bad - how are you?', additional_kwargs={}, example=False), AIMessage(content=\"I'm an AI chatbot, so I don't have feelings, but I'm here to assist you. How can I help you today?\", additional_kwargs={}, example=False)]\n",
      "Human:Answer briefly. What are the first 3 colors of a rainbow?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The first three colors of a rainbow are red, orange, and yellow.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm_chain.predict(human_input=\"Answer briefly. What are the first 3 colors of a rainbow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi there my friend', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False)]) output_key=None input_key=None return_messages=True human_prefix='Human' ai_prefix='AI' memory_key='chat_history'\n"
     ]
    }
   ],
   "source": [
    "print(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "[HumanMessage(content='Hi there my friend', additional_kwargs={}, example=False), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False), HumanMessage(content='Not too bad - how are you?', additional_kwargs={}, example=False), AIMessage(content=\"I'm an AI chatbot, so I don't have feelings, but I'm here to assist you. How can I help you today?\", additional_kwargs={}, example=False), HumanMessage(content='Answer briefly. What are the first 3 colors of a rainbow?', additional_kwargs={}, example=False), AIMessage(content='The first three colors of a rainbow are red, orange, and yellow.', additional_kwargs={}, example=False)]\n",
      "Human:And the next 4?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The next four colors of a rainbow are green, blue, indigo, and violet.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm_chain.predict(human_input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
